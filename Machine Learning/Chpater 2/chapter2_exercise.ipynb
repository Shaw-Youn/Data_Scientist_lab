{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42112901",
   "metadata": {},
   "source": [
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c42c7",
   "metadata": {},
   "source": [
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\"></script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70033af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9087c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c87ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d343f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6364797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector,make_column_transformer,ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed01e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR,SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236f81f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b2432ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b753d3",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVMs) are a versatile class of supervised learning algorithms used for both classification (SVC - Support Vector Classification) and regression (SVR - Support Vector Regression). The performance of SVM models heavily depends on the choice of the **kernel function**, which transforms the input data into a higher-dimensional space to make it separable or to fit a more complex function.\n",
    "\n",
    "### **List of Kernel Types for SVM (SVR & SVC):**\n",
    "\n",
    "#### **1. Linear Kernel**\n",
    "   - Formula: $ K(x_i, x_j) = x_i^T x_j $\n",
    "   - Used when data is linearly separable.\n",
    "   - Fast and works well for high-dimensional data.\n",
    "   - **Use Case**: Text classification, linear regression.\n",
    "\n",
    "#### **2. Polynomial Kernel**\n",
    "   - Formula: $K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d $\n",
    "   - Parameters:\n",
    "     - \\( $\\gamma$ ) (gamma): Controls influence of each training example.\n",
    "     - \\( $d$ ) (degree): Degree of the polynomial.\n",
    "     - \\( $r$ ) (coef0): Independent term.\n",
    "   - **Use Case**: Moderate non-linearity, image processing.\n",
    "\n",
    "#### **3. Radial Basis Function (RBF) / Gaussian Kernel**\n",
    "   - Formula: $ K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2) $\n",
    "   - Most commonly used kernel.\n",
    "   - Parameter:\n",
    "     - ( $\\gamma $): Controls the \"spread\" of the kernel (inverse of variance).\n",
    "   - **Use Case**: Highly non-linear problems, default choice for many tasks.\n",
    "\n",
    "#### **4. Sigmoid Kernel (Hyperbolic Tangent Kernel)**\n",
    "   - Formula: $K(x_i, x_j) = \\tanh(\\gamma x_i^T x_j + r) $\n",
    "   - Similar to neural network activation functions.\n",
    "   - Parameters:\n",
    "     - ( $\\gamma $): Scaling factor.\n",
    "     - ( $r$): Bias term.\n",
    "   - **Use Case**: Neural network-like models, but less commonly used than RBF.\n",
    "\n",
    "#### **5. Custom Kernels**\n",
    "   - Users can define their own kernel functions as long as they satisfy **Mercer’s condition** (must be positive semi-definite).\n",
    "   - Example: String kernels for text, graph kernels for structured data.\n",
    "\n",
    "### **Summary Table of Kernels in SVM (SVC & SVR)**\n",
    "\n",
    "| **Kernel Type** | **Formula** | **Key Parameters** | **Best For** |\n",
    "|----------------|------------|-------------------|-------------|\n",
    "| **Linear** | $$K(x_i, x_j) = x_i^T x_j $$| None | Linear problems, high-dimensional data |\n",
    "| **Polynomial** | $$ K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d $$ | `degree (d)`, `gamma (γ)`, `coef0 (r)` | Moderate non-linearity |\n",
    "| **RBF (Gaussian)** | $$K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2) $$| `gamma (γ)` | Highly non-linear data (default choice) \n",
    "| **Sigmoid** | $$K(x_i, x_j) = \\tanh (\\gamma x_i^T x_j + r) $$| `gamma (γ)`, `coef0 (r)` | Neural network-like models |\n",
    "| **Custom** | User-defined | Depends on implementation | Specialized problems |\n",
    "\n",
    "### **Which Kernel to Choose?**\n",
    "- **Linear Kernel**: Best for large feature spaces (e.g., text classification).\n",
    "- **RBF Kernel**: Default choice for most non-linear problems.\n",
    "- **Polynomial Kernel**: Useful when features have multiplicative interactions.\n",
    "- **Sigmoid Kernel**: Rarely used, but can mimic neural networks.\n",
    "\n",
    "In `scikit-learn`, these kernels can be used in `SVC` and `SVR` via the `kernel` parameter:\n",
    "```python\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "# Example: RBF Kernel for SVC\n",
    "model = SVC(kernel='rbf', gamma=0.1)\n",
    "\n",
    "# Example: Polynomial Kernel for SVR\n",
    "model = SVR(kernel='poly', degree=3, gamma='scale')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ac809",
   "metadata": {},
   "source": [
    "### **Understanding Hyperparameters in SVM Kernels (SVC/SVR)**\n",
    "When using kernels like `'linear'`, `'poly'` (polynomial), `'rbf'` (Radial Basis Function), and `'sigmoid'` in SVM, the choice of **hyperparameters** significantly impacts model performance. Below is a breakdown of key hyperparameters for each kernel and how they affect the decision boundary or regression fit.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Common Hyperparameters Across All Kernels**\n",
    "These hyperparameters are shared by all kernels in `scikit-learn`'s `SVC` and `SVR`:\n",
    "\n",
    "| Hyperparameter | Role | Default Value | Impact |\n",
    "|--------------|------|--------------|--------|\n",
    "| **`C`** | Regularization parameter | `1.0` | - **Small `C`**: More margin, allows misclassification (underfitting).<br>- **Large `C`**: Stricter margin, may overfit. |\n",
    "| **`epsilon`** (SVR only) | Sensitivity to errors | `0.1` | - Larger `epsilon` → wider \"tube\" (more error tolerance).<br>- Smaller `epsilon` → stricter fit. |\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Kernel-Specific Hyperparameters**\n",
    "Each kernel has unique hyperparameters that control its flexibility.\n",
    "\n",
    "### **A. Linear Kernel (`kernel='linear'`)**\n",
    "- **No additional hyperparameters** (just `C`).\n",
    "- **Decision boundary**: A straight line (or hyperplane in high dimensions).\n",
    "- **Use case**: When data is (near) linearly separable.\n",
    "\n",
    "### **B. Polynomial Kernel (`kernel='poly'`)**\n",
    "| Hyperparameter | Role | Default | Impact |\n",
    "|--------------|------|--------|--------|\n",
    "| **`degree`** | Polynomial degree | `3` | - Higher `degree` → more complex curves (risk of overfitting). |\n",
    "| **`gamma`** | Kernel coefficient | `'scale'` | - High `gamma` → sharper influence of each sample (risk of overfitting). |\n",
    "| **`coef0`** | Independent term | `0.0` | - Controls bias (`r` in kernel formula). |\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='poly', degree=3, gamma='scale', coef0=1, C=1.0)\n",
    "```\n",
    "\n",
    "### **C. RBF (Gaussian) Kernel (`kernel='rbf'`)**\n",
    "| Hyperparameter | Role | Default | Impact |\n",
    "|--------------|------|--------|--------|\n",
    "| **`gamma`** | Inverse kernel width | `'scale'` | - **Low `gamma`**: Smooth decision boundary (underfitting).<br>- **High `gamma`**: Tight fit around points (overfitting). |\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from sklearn.svm import SVR\n",
    "model = SVR(kernel='rbf', gamma=0.1, C=10.0, epsilon=0.2)\n",
    "```\n",
    "\n",
    "### **D. Sigmoid Kernel (`kernel='sigmoid'`)**\n",
    "| Hyperparameter | Role | Default | Impact |\n",
    "|--------------|------|--------|--------|\n",
    "| **`gamma`** | Scaling factor | `'scale'` | - Similar to RBF. |\n",
    "| **`coef0`** | Bias term | `0.0` | - Shifts the activation threshold. |\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "model = SVC(kernel='sigmoid', gamma=0.01, coef0=1, C=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. How to Tune Hyperparameters?**\n",
    "### **A. For `C` (Regularization)**\n",
    "- Start with `C=1.0` and test `[0.01, 0.1, 1, 10, 100]`.\n",
    "- **High `C`** → Low bias, high variance (overfitting).\n",
    "- **Low `C`** → High bias, low variance (underfitting).\n",
    "\n",
    "### **B. For `gamma` (RBF/Poly/Sigmoid)**\n",
    "- `gamma='scale'` (default) uses `1 / (n_features * X.var())`.\n",
    "- Try `[0.001, 0.01, 0.1, 1, 10]`.\n",
    "- **High `gamma`** → Overfitting (tight fit).\n",
    "- **Low `gamma`** → Underfitting (smoother fit).\n",
    "\n",
    "### **C. For `degree` (Polynomial Kernel)**\n",
    "- Start with `degree=2` or `3`.\n",
    "- Higher degrees risk overfitting.\n",
    "\n",
    "### **D. For `epsilon` (SVR)**\n",
    "- Controls the width of the \"insensitive\" tube.\n",
    "- Try `[0.01, 0.1, 0.5, 1.0]`.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Practical Recommendations**\n",
    "| Kernel | Best For | Key Hyperparameters | Tuning Strategy |\n",
    "|--------|---------|---------------------|-----------------|\n",
    "| **Linear** | High-dimensional data | `C` | Adjust `C` for bias-variance tradeoff. |\n",
    "| **Polynomial** | Moderate non-linearity | `degree`, `gamma`, `coef0` | Start with `degree=3`, tune `gamma`. |\n",
    "| **RBF** | Default for non-linear | `gamma`, `C` | Use `GridSearchCV` on `gamma` and `C`. |\n",
    "| **Sigmoid** | Rarely used | `gamma`, `coef0` | Similar to RBF but less stable. |\n",
    "\n",
    "### **Example: Using `GridSearchCV` for RBF Kernel**\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.01, 0.1, 1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Summary**\n",
    "- **`C`**: Controls regularization (higher = stricter fit).\n",
    "- **`gamma`**: Controls kernel width (higher = tighter fit).\n",
    "- **`degree`**: Only for polynomial kernel (higher = more curves).\n",
    "- **`coef0`**: Bias term in polynomial/sigmoid kernels.\n",
    "- **`epsilon`**: Only for SVR (tolerance for errors).\n",
    "\n",
    "For most cases, **RBF (`kernel='rbf'`)** with tuned `C` and `gamma` works best. Use `GridSearchCV` for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c91e7",
   "metadata": {},
   "source": [
    "### **Correct Interpretation of `C` and `gamma` for Overfitting/Underfitting**  \n",
    "Here’s a precise breakdown of how `C` and `gamma` control overfitting/underfitting in SVM kernels (RBF, Polynomial, etc.):\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Regularization Parameter (`C`)**\n",
    "- **Role**: Controls the trade-off between **maximizing margin width** and **minimizing classification errors**.  \n",
    "- **Impact**:  \n",
    "  - **High `C`** (e.g., `C=100`):  \n",
    "    - The model **penalizes misclassifications more heavily**.  \n",
    "    - Result: **Tighter decision boundary** (may overfit if `C` is too high).  \n",
    "  - **Low `C`** (e.g., `C=0.01`):  \n",
    "    - The model **allows more misclassifications** for a wider margin.  \n",
    "    - Result: **Simpler (smoother) decision boundary** (may underfit if `C` is too low).  \n",
    "\n",
    "#### **How to Adjust `C`?**\n",
    "| Scenario           | Action       | Reason                                                                 |\n",
    "|--------------------|--------------|------------------------------------------------------------------------|\n",
    "| **Overfitting**    | Decrease `C` | Reduces strictness, allowing more errors for a broader margin.         |\n",
    "| **Underfitting**   | Increase `C` | Makes the model stricter, fitting training data more closely.          |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Kernel Coefficient (`gamma`)**\n",
    "- **Role**: Defines how far the influence of a single training example reaches (only for **RBF**, **Poly**, **Sigmoid** kernels).  \n",
    "  - **Low `gamma`**:  \n",
    "    - The kernel has a **wide influence** (smooth decision boundary).  \n",
    "    - Similar to a **linear model** (may underfit).  \n",
    "  - **High `gamma`**:  \n",
    "    - The kernel has a **narrow influence** (tight fit around data points).  \n",
    "    - May capture **noise** (overfitting).  \n",
    "\n",
    "#### **How to Adjust `gamma`?**\n",
    "| Scenario           | Action          | Reason                                                                 |\n",
    "|--------------------|-----------------|------------------------------------------------------------------------|\n",
    "| **Overfitting**    | Decrease `gamma`| Makes the kernel smoother, reducing sensitivity to noise.             |\n",
    "| **Underfitting**   | Increase `gamma`| Allows the kernel to fit complex patterns (but risk overfitting).      |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Quick Reference Table**\n",
    "| Hyperparameter | High Value → | Low Value → | Overfitting Fix | Underfitting Fix |\n",
    "|---------------|-------------|-------------|-----------------|------------------|\n",
    "| **`C`**      | Overfitting | Underfitting| **Decrease `C`** | **Increase `C`** |\n",
    "| **`gamma`**  | Overfitting | Underfitting| **Decrease `gamma`** | **Increase `gamma`** |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Practical Example (RBF Kernel)**\n",
    "#### **Case 1: Overfitting (High `C` + High `gamma`)**\n",
    "- Symptoms:  \n",
    "  - Training accuracy ≈ 100%, but validation accuracy is poor.  \n",
    "  - Decision boundary is **too complex** (fits noise).  \n",
    "- Fix:  \n",
    "  ```python\n",
    "  model = SVC(kernel='rbf', C=0.1, gamma=0.01)  # Reduce both C and gamma\n",
    "  ```\n",
    "\n",
    "#### **Case 2: Underfitting (Low `C` + Low `gamma`)**\n",
    "- Symptoms:  \n",
    "  - Poor performance on **both training and validation data**.  \n",
    "  - Decision boundary is **too smooth** (misses patterns).  \n",
    "- Fix:  \n",
    "  ```python\n",
    "  model = SVC(kernel='rbf', C=10, gamma=1.0)  # Increase both C and gamma\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Pro Tips**\n",
    "1. **Default Values**:  \n",
    "   - `C=1.0`, `gamma='scale'` (auto-scales based on data variance).  \n",
    "   - Start with defaults, then tune.  \n",
    "\n",
    "2. **Use `GridSearchCV` for Automation**:  \n",
    "   ```python\n",
    "   from sklearn.model_selection import GridSearchCV\n",
    "   param_grid = {\n",
    "       'C': [0.1, 1, 10],\n",
    "       'gamma': [0.01, 0.1, 1]\n",
    "   }\n",
    "   grid = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)\n",
    "   grid.fit(X_train, y_train)\n",
    "   print(\"Best params:\", grid.best_params_)\n",
    "   ```\n",
    "\n",
    "3. **Visualize Decision Boundaries**:  \n",
    "   - Use libraries like `mlxtend` to plot how `C` and `gamma` affect the model.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **`C`** controls **how strictly misclassifications are penalized**.  \n",
    "- **`gamma`** controls **how far the influence of a single point reaches**.  \n",
    "- **Overfitting?** → Lower `C` and/or `gamma`.  \n",
    "- **Underfitting?** → Increase `C` and/or `gamma`.  \n",
    "\n",
    "This aligns with your intuition! Let me know if you'd like a deeper dive into any specific kernel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
