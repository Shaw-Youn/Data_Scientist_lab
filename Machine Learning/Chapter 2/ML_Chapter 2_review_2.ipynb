{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e1bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab618c5",
   "metadata": {},
   "source": [
    "**(A)** The `SelectFromModel` class in `scikit-learn` is a **feature selection** transformer that selects features based on the importance scores assigned by a machine learning model. It is particularly useful when you want to reduce the dimensionality of your dataset by keeping only the most important features according to a trained model.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Does `SelectFromModel` Work?**\n",
    "1. **Train a Model**:\n",
    "   - You first train a model (e.g., a linear regression, decision tree, or any model that provides feature importance scores).\n",
    "\n",
    "2. **Extract Feature Importance**:\n",
    "   - The model assigns importance scores to each feature (e.g., coefficients in linear models or feature importance in tree-based models).\n",
    "\n",
    "3. **Select Features**:\n",
    "   - `SelectFromModel` uses a threshold (or other criteria) to select features whose importance scores meet the specified condition.\n",
    "\n",
    "4. **Transform the Dataset**:\n",
    "   - The transformer reduces the dataset to only the selected features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Parameters of `SelectFromModel`**\n",
    "Here are the most important parameters of `SelectFromModel`:\n",
    "\n",
    "1. **`estimator`**:\n",
    "   - The model used to compute feature importance scores.\n",
    "   - Example: `LinearRegression()`, `RandomForestClassifier()`, etc.\n",
    "\n",
    "2. **`threshold`** (optional):\n",
    "   - The threshold value for feature selection.\n",
    "   - Features with importance scores greater than this threshold are selected.\n",
    "   - If `threshold` is not specified, the default behavior is to use the mean of the importance scores.\n",
    "\n",
    "3. **`prefit`** (optional):\n",
    "   - If `True`, the estimator is assumed to be already fitted.\n",
    "   - If `False` (default), the estimator is fitted on the data provided to `SelectFromModel`.\n",
    "\n",
    "4. **`max_features`** (optional):\n",
    "   - The maximum number of features to select.\n",
    "   - If specified, only the top `max_features` features are selected.\n",
    "\n",
    "5. **`importance_getter`** (optional):\n",
    "   - A function or string to extract feature importance from the estimator.\n",
    "   - Default is `'auto'`, which automatically detects the importance attribute (e.g., `coef_` for linear models or `feature_importances_` for tree-based models).\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Use `SelectFromModel`**\n",
    "Here’s a step-by-step example of how to use `SelectFromModel`:\n",
    "\n",
    "#### **Step 1: Import Libraries**\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "```\n",
    "\n",
    "#### **Step 2: Load Dataset**\n",
    "```python\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "```\n",
    "\n",
    "#### **Step 3: Split Data into Training and Testing Sets**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "#### **Step 4: Train a Model and Use `SelectFromModel`**\n",
    "```python\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Use SelectFromModel to select features\n",
    "selector = SelectFromModel(estimator=model, threshold='median')  # Use median importance as threshold\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the dataset to keep only selected features\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "```\n",
    "\n",
    "#### **Step 5: Train and Evaluate the Model on Selected Features**\n",
    "```python\n",
    "# Train the model on the selected features\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with selected features: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "#### **Step 6: Inspect Selected Features**\n",
    "```python\n",
    "# Get the indices of the selected features\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "print(\"Selected feature indices:\", selected_feature_indices)\n",
    "\n",
    "# Get the names of the selected features (if available)\n",
    "if hasattr(data, 'feature_names'):\n",
    "    selected_feature_names = data.feature_names[selected_feature_indices]\n",
    "    print(\"Selected feature names:\", selected_feature_names)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**\n",
    "1. **Feature Importance**:\n",
    "   - The model used in `SelectFromModel` must provide feature importance scores (e.g., `coef_` for linear models or `feature_importances_` for tree-based models).\n",
    "\n",
    "2. **Threshold**:\n",
    "   - You can specify a threshold to select features. If not specified, the default is the mean of the importance scores.\n",
    "\n",
    "3. **Dimensionality Reduction**:\n",
    "   - `SelectFromModel` reduces the number of features, which can improve model performance, reduce overfitting, and speed up training.\n",
    "\n",
    "4. **Flexibility**:\n",
    "   - You can use any model that provides feature importance scores, making `SelectFromModel` highly flexible.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Output**\n",
    "```\n",
    "Accuracy with selected features: 0.96\n",
    "Selected feature indices: [ 1  2  3  6  7 13 20 22 23 24 25 26 27]\n",
    "Selected feature names: ['mean texture' 'mean perimeter' 'mean area' 'mean smoothness' 'mean compactness' ...]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use `SelectFromModel`**\n",
    "- When you want to perform feature selection based on a model's feature importance scores.\n",
    "- When you have a high-dimensional dataset and want to reduce the number of features.\n",
    "- When you want to improve model performance by removing irrelevant or redundant features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages**\n",
    "- Simple and easy to use.\n",
    "- Works with any model that provides feature importance scores.\n",
    "- Can significantly reduce the dimensionality of the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations**\n",
    "- The quality of feature selection depends on the model used.\n",
    "- If the model is not well-suited for the data, the selected features may not be optimal.\n",
    "\n",
    "By using `SelectFromModel`, you can efficiently select the most important features for your machine learning tasks, leading to better model performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726536e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8dd7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ad1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538c68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Use SelectFromModel to select features\n",
    "selector = SelectFromModel(estimator=model, threshold='median')  # Use median importance as threshold\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the dataset to keep only selected features\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa7810a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with selected features: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the selected features\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with selected features: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6036d2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature indices: [ 0  2  3  5  6  7 10 13 20 21 22 23 25 26 27]\n",
      "Selected feature names: ['mean radius' 'mean perimeter' 'mean area' 'mean compactness'\n",
      " 'mean concavity' 'mean concave points' 'radius error' 'area error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst compactness' 'worst concavity' 'worst concave points']\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the selected features\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "print(\"Selected feature indices:\", selected_feature_indices)\n",
    "\n",
    "# Get the names of the selected features (if available)\n",
    "if hasattr(data, 'feature_names'):\n",
    "    selected_feature_names = data.feature_names[selected_feature_indices]\n",
    "    print(\"Selected feature names:\", selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace44e95",
   "metadata": {},
   "source": [
    "**(B)** Combining the `SelectFromModel` feature selector and your machine learning model into a **pipeline** is a great idea. This ensures that the feature selection and model training are performed seamlessly during cross-validation or testing, avoiding data leakage and simplifying your code. You can achieve this using `scikit-learn`'s `Pipeline`.\n",
    "\n",
    "Here’s how you can do it:\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Guide**\n",
    "\n",
    "#### **1. Import Libraries**\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "```\n",
    "\n",
    "#### **2. Load Dataset**\n",
    "```python\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "```\n",
    "\n",
    "#### **3. Split Data into Training and Testing Sets**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "#### **4. Create a Pipeline**\n",
    "Combine the `SelectFromModel` feature selector and the model into a pipeline:\n",
    "```python\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the SelectFromModel transformer\n",
    "selector = SelectFromModel(estimator=model, threshold='median')\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', selector),  # Step 1: Feature selection\n",
    "    ('classification', model)         # Step 2: Classification\n",
    "])\n",
    "```\n",
    "\n",
    "#### **5. Train the Pipeline**\n",
    "Fit the pipeline on the training data:\n",
    "```python\n",
    "pipeline.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### **6. Make Predictions**\n",
    "Use the pipeline to make predictions on the test data:\n",
    "```python\n",
    "y_pred = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "#### **7. Evaluate the Model**\n",
    "Calculate the accuracy of the model:\n",
    "```python\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with pipeline: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "#### **8. Inspect Selected Features (Optional)**\n",
    "If you want to inspect which features were selected, you can access the `SelectFromModel` step from the pipeline:\n",
    "```python\n",
    "selected_feature_indices = pipeline.named_steps['feature_selection'].get_support(indices=True)\n",
    "print(\"Selected feature indices:\", selected_feature_indices)\n",
    "\n",
    "if hasattr(data, 'feature_names'):\n",
    "    selected_feature_names = data.feature_names[selected_feature_indices]\n",
    "    print(\"Selected feature names:\", selected_feature_names)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Full Code Example**\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(estimator=model, threshold='median')),\n",
    "    ('classification', model)\n",
    "])\n",
    "\n",
    "# Train pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with pipeline: {accuracy:.2f}\")\n",
    "\n",
    "# Inspect selected features\n",
    "selected_feature_indices = pipeline.named_steps['feature_selection'].get_support(indices=True)\n",
    "print(\"Selected feature indices:\", selected_feature_indices)\n",
    "\n",
    "if hasattr(data, 'feature_names'):\n",
    "    selected_feature_names = data.feature_names[selected_feature_indices]\n",
    "    print(\"Selected feature names:\", selected_feature_names)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Benefits of Using a Pipeline**\n",
    "1. **Avoid Data Leakage**:\n",
    "   - The feature selection step is performed within the pipeline, ensuring that the test data is not used during feature selection.\n",
    "\n",
    "2. **Simpler Code**:\n",
    "   - You don’t need to manually fit and transform the data for feature selection.\n",
    "\n",
    "3. **Easier Cross-Validation**:\n",
    "   - You can use the pipeline directly with `GridSearchCV` or `RandomizedSearchCV` for hyperparameter tuning.\n",
    "\n",
    "4. **Reproducibility**:\n",
    "   - The entire process (feature selection + model training) is encapsulated in a single object, making it easier to reproduce results.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Output**\n",
    "```\n",
    "Accuracy with pipeline: 0.96\n",
    "Selected feature indices: [ 1  2  3  6  7 13 20 22 23 24 25 26 27]\n",
    "Selected feature names: ['mean texture' 'mean perimeter' 'mean area' 'mean smoothness' 'mean compactness' ...]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Using the Pipeline with Cross-Validation**\n",
    "If you want to perform cross-validation, you can use the pipeline directly with `cross_val_score` or `GridSearchCV`. For example:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation accuracy: {scores.mean():.2f} (±{scores.std():.2f})\")\n",
    "```\n",
    "\n",
    "This approach ensures that feature selection is performed independently for each fold, maintaining the integrity of the validation process.\n",
    "\n",
    "---\n",
    "\n",
    "By combining `SelectFromModel` and your model into a pipeline, you streamline the workflow, improve reproducibility, and avoid common pitfalls like data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae9876",
   "metadata": {},
   "source": [
    " **(C)** The `SelectFromModel` class in `scikit-learn` is a **feature selection transformer** that works with models capable of providing **feature importance scores** or **coefficients**. However, not all models are compatible with `SelectFromModel` out of the box. Let’s break this down:\n",
    "\n",
    "---\n",
    "\n",
    "### **Models Compatible with `SelectFromModel`**\n",
    "`SelectFromModel` works with models that have one of the following attributes:\n",
    "1. **`feature_importances_`**:\n",
    "   - Tree-based models like `DecisionTreeClassifier`, `RandomForestClassifier`, `GradientBoostingClassifier`, etc., provide feature importance scores.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.ensemble import RandomForestClassifier\n",
    "     model = RandomForestClassifier()\n",
    "     ```\n",
    "\n",
    "2. **`coef_`**:\n",
    "   - Linear models like `LinearRegression`, `LogisticRegression`, `Lasso`, `Ridge`, etc., provide coefficients that can be used as importance scores.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.linear_model import LogisticRegression\n",
    "     model = LogisticRegression()\n",
    "     ```\n",
    "\n",
    "3. **Custom Importance Getters**:\n",
    "   - If a model does not provide `feature_importances_` or `coef_` by default, you can use the `importance_getter` parameter to specify a custom function to extract feature importance scores.\n",
    "\n",
    "---\n",
    "\n",
    "### **Models That Are NOT Directly Compatible**\n",
    "Some models, like **Support Vector Machines (SVC)**, do not inherently provide feature importance scores or coefficients that can be directly used by `SelectFromModel`. For example:\n",
    "- **`SVC`** (Support Vector Classifier) does not provide `feature_importances_` or `coef_` unless it is a linear kernel SVM.\n",
    "- Non-linear SVMs (e.g., RBF kernel) do not provide feature importance scores because they operate in a transformed feature space.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Use `SelectFromModel` with Different Models**\n",
    "\n",
    "#### **1. Tree-Based Models (e.g., Decision Trees, Random Forests)**\n",
    "Tree-based models are fully compatible with `SelectFromModel` because they provide `feature_importances_`:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "selector = SelectFromModel(model, threshold='median')\n",
    "selector.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### **2. Linear Models (e.g., Logistic Regression, Lasso)**\n",
    "Linear models are compatible because they provide `coef_`:\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = LogisticRegression()\n",
    "selector = SelectFromModel(model, threshold='median')\n",
    "selector.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### **3. Support Vector Machines (SVC)**\n",
    "- **Linear Kernel SVC**:\n",
    "  If you use a linear kernel, SVC provides `coef_`, so it can work with `SelectFromModel`:\n",
    "  ```python\n",
    "  from sklearn.svm import SVC\n",
    "  from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "  model = SVC(kernel='linear')\n",
    "  selector = SelectFromModel(model, threshold='median')\n",
    "  selector.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Non-Linear Kernel SVC**:\n",
    "  Non-linear kernels (e.g., RBF) do not provide feature importance scores. In this case, you cannot directly use `SelectFromModel`. Instead, you can:\n",
    "  - Use other feature selection methods (e.g., `SelectKBest`, `RFE`).\n",
    "  - Use a wrapper method like `RFE` (Recursive Feature Elimination) with SVC.\n",
    "\n",
    "---\n",
    "\n",
    "### **Custom Importance Getters**\n",
    "If a model does not provide `feature_importances_` or `coef_`, you can define a custom function to extract importance scores and pass it to `SelectFromModel` using the `importance_getter` parameter. For example:\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Define a custom importance getter\n",
    "def custom_importance_getter(model):\n",
    "    return model.coef_[0]  # Example for linear SVC\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "selector = SelectFromModel(model, importance_getter=custom_importance_getter)\n",
    "selector.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Works with**: Models that provide `feature_importances_` (e.g., tree-based models) or `coef_` (e.g., linear models).\n",
    "- **Does not work with**: Models that do not provide feature importance scores or coefficients (e.g., non-linear SVC, k-Nearest Neighbors).\n",
    "- **Workaround**: Use custom importance getters or alternative feature selection methods like `RFE` or `SelectKBest`.\n",
    "\n",
    "If you are using a model like **SVC with a non-linear kernel**, you may need to explore other feature selection techniques or switch to a model that provides feature importance scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
