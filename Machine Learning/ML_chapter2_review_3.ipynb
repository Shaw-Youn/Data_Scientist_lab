{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8052fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf29abe",
   "metadata": {},
   "source": [
    "`joblib` is a popular library in Python for efficiently **saving and loading** large data objects, particularly NumPy arrays and scikit-learn models. It is often used to save trained machine learning models to disk and load them later for inference or further training. Below is a step-by-step guide on how to use `joblib` to save and load trained models.\n",
    "\n",
    "### Installation\n",
    "First, ensure you have `joblib` installed. You can install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install joblib\n",
    "```\n",
    "\n",
    "### Saving a Trained Model\n",
    "After training a model (e.g., using scikit-learn), you can save it to a file using `joblib.dump`.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Load dataset and split into training and testing sets\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(model, 'trained_model.pkl')\n",
    "```\n",
    "\n",
    "### Loading a Saved Model\n",
    "To load the saved model from the file, use `joblib.load`.\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load the model from the file\n",
    "loaded_model = joblib.load('trained_model.pkl')\n",
    "\n",
    "# Use the loaded model for prediction\n",
    "predictions = loaded_model.predict(X_test)\n",
    "print(predictions)\n",
    "```\n",
    "\n",
    "### Key Points\n",
    "1. **File Extension**: While `.pkl` is commonly used, you can use any file extension (or none). `joblib` doesn't enforce a specific extension.\n",
    "2. **Compression**: `joblib.dump` supports compression to reduce file size. You can specify compression methods like `gzip`, `bz2`, or `lzma`.\n",
    "\n",
    "   ```python\n",
    "   joblib.dump(model, 'trained_model.pkl.gz', compress='gzip')\n",
    "   ```\n",
    "\n",
    "3. **Efficiency**: `joblib` is optimized for large NumPy arrays, making it faster and more efficient than Python's built-in `pickle` for scikit-learn models.\n",
    "4. **Cross-Platform**: Saved models can be loaded on different machines or platforms, as long as the same version of the libraries is used.\n",
    "\n",
    "### Example with Compression\n",
    "Hereâ€™s an example of saving and loading a model with compression:\n",
    "\n",
    "```python\n",
    "# Save with compression\n",
    "joblib.dump(model, 'trained_model.pkl.gz', compress='gzip')\n",
    "\n",
    "# Load the compressed model\n",
    "loaded_model = joblib.load('trained_model.pkl.gz')\n",
    "```\n",
    "\n",
    "### Notes\n",
    "- Ensure compatibility between the versions of `joblib`, scikit-learn, and other libraries used during training and loading.\n",
    "- If you encounter issues with large models, consider using `joblib`'s `dump` and `load` with memory-mapped arrays for better performance.\n",
    "-  when loading a trained model using `joblib` (or `pickle`), **you must ensure that all custom classes, functions, and dependencies** used during the training of the model are available in the environment where you are loading the model. This is because the serialization process (saving the model) stores the structure of the object but not the actual code. When deserializing (loading the model), Python needs access to the original code to reconstruct the object.\n",
    "\n",
    "### Why Are Custom Classes/Functions Required?\n",
    "When you save a trained model, `joblib` serializes the model's parameters, architecture, and other necessary data. However, it does not save the actual Python code (e.g., custom classes, functions, or transformers) that defines the model or any preprocessing steps. Therefore, when you load the model, Python needs access to the original code to properly reconstruct the object.\n",
    "\n",
    "### Steps to Load and Use a Trained Model with Custom Dependencies\n",
    "1. **Ensure All Dependencies Are Available**:\n",
    "   - Import all custom classes, functions, and libraries that were used during the training process.\n",
    "   - Ensure that the versions of the libraries (e.g., scikit-learn, TensorFlow, etc.) are compatible with the ones used during training.\n",
    "\n",
    "2. **Load the Model**:\n",
    "   - Use `joblib.load` to load the saved model.\n",
    "\n",
    "3. **Make Predictions**:\n",
    "   - Use the loaded model to make predictions.\n",
    "\n",
    "### Example Scenario\n",
    "Suppose you trained a model that uses a custom preprocessing class and a custom metric function. Here's how you would handle loading and using the model:\n",
    "\n",
    "#### Custom Code Used During Training\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "# Custom preprocessing class\n",
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, factor=1.0):\n",
    "        self.factor = factor\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X * self.factor\n",
    "\n",
    "# Custom metric function\n",
    "def custom_metric(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "```\n",
    "\n",
    "#### Training and Saving the Model\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with the custom scaler\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', CustomScaler(factor=2.0)),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(pipeline, 'trained_model.pkl')\n",
    "```\n",
    "\n",
    "#### Loading and Using the Model\n",
    "```python\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Re-define the custom classes/functions (or import them from the original module)\n",
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, factor=1.0):\n",
    "        self.factor = factor\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X * self.factor\n",
    "\n",
    "def custom_metric(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "# Load the trained model\n",
    "pipeline = joblib.load('trained_model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipeline.predict(X_test)\n",
    "print(predictions)\n",
    "```\n",
    "\n",
    "### Key Points to Remember\n",
    "1. **Recreate the Environment**:\n",
    "   - Ensure that the environment where you load the model has all the necessary dependencies, including custom classes, functions, and libraries.\n",
    "\n",
    "2. **Version Compatibility**:\n",
    "   - Use the same versions of libraries (e.g., scikit-learn, TensorFlow) as were used during training to avoid compatibility issues.\n",
    "\n",
    "3. **Organize Code**:\n",
    "   - If you have many custom classes or functions, consider organizing them into a separate module and importing them during both training and inference.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - If you forget to import a custom class or function, you'll get an error like:\n",
    "     ```\n",
    "     AttributeError: Can't get attribute 'CustomScaler' on <module '__main__'>\n",
    "     ```\n",
    "     This indicates that Python cannot find the required class or function.\n",
    "\n",
    "By ensuring that all dependencies are available, you can successfully load and use your trained model for predictions.\n",
    "\n",
    "By following these steps, you can easily save and load trained machine learning models using `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce38f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
