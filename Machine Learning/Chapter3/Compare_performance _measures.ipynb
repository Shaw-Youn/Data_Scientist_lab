{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8064501e",
   "metadata": {},
   "source": [
    "### **When to Use Recall vs. Precision?**  \n",
    "The choice between **recall** (sensitivity) and **precision** depends on the **cost of false negatives (FN) vs. false positives (FP)** in your application. Here’s a simple guide:\n",
    "\n",
    "| **Metric**  | **Focus**               | **When to Prioritize**                                  | **Example Use Cases**                     |\n",
    "|-------------|-------------------------|-------------------------------------------------------|------------------------------------------|\n",
    "| **Recall**  | Minimize **False Negatives (FN)** | When missing a positive case is **dangerous/costly**. | Cancer detection, spam filtering (security), fraud detection |\n",
    "| **Precision** | Minimize **False Positives (FP)** | When false alarms are **harmful/expensive**.          | Email spam (business), recommendation systems, legal document review |\n",
    "\n",
    "---\n",
    "\n",
    "### **1. When to Prioritize Recall (High Recall)**\n",
    "**Goal:** Catch **as many true positives as possible**, even if it means some false alarms.  \n",
    "**Use cases:**  \n",
    "- **Medical Diagnosis (Cancer, HIV):** Missing a case (FN) could be fatal.  \n",
    "- **Spam Detection (Security):** Letting phishing emails (FN) into the inbox is risky.  \n",
    "- **Fraud Detection:** Missing fraud (FN) costs money.  \n",
    "- **Search & Rescue:** Missing a survivor (FN) is unacceptable.  \n",
    "\n",
    "**Trade-off:** Higher recall often means **more false positives (FP)**.  \n",
    "\n",
    "**Example:**  \n",
    "- A cancer test with **90% recall** catches 90% of cancers but may flag some healthy patients (FP).  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. When to Prioritize Precision (High Precision)**\n",
    "**Goal:** Ensure **positive predictions are correct**, even if some true positives are missed.  \n",
    "**Use cases:**  \n",
    "- **Email Spam (Business):** Incorrectly blocking important emails (FP) is worse than some spam slipping through.  \n",
    "- **Recommendation Systems:** Showing irrelevant products (FP) hurts user trust.  \n",
    "- **Legal/Financial Docs:** Wrongly flagging a legal doc as fraudulent (FP) causes delays.  \n",
    "- **Autonomous Vehicles:** False alarms (FP) could make the car brake unnecessarily.  \n",
    "\n",
    "**Trade-off:** Higher precision often means **more false negatives (FN)**.  \n",
    "\n",
    "**Example:**  \n",
    "- A spam filter with **95% precision** rarely mislabels good emails as spam but may let some spam through (FN).  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. When to Balance Both (F1-Score)**\n",
    "If both **FN and FP** are important, use the **F1-score** (harmonic mean of precision and recall).  \n",
    "\n",
    "**Use cases:**  \n",
    "- **Moderate-risk scenarios** (e.g., customer churn prediction, defect detection).  \n",
    "- **When no single error type is drastically worse** than the other.  \n",
    "\n",
    "**Example:**  \n",
    "- A social media content filter balances **removing harmful posts (recall)** and **avoiding over-censorship (precision)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Practical Decision Flow**  \n",
    "Ask:  \n",
    "1. **Is a missed detection (FN) worse than a false alarm (FP)?** → **Recall**.  \n",
    "   - *Example:* Missing cancer is worse than a false scare.  \n",
    "2. **Is a false alarm (FP) worse than a missed case (FN)?** → **Precision**.  \n",
    "   - *Example:* Blocking a legitimate email is worse than missing spam.  \n",
    "3. **Are both important?** → **F1-score or adjust threshold**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **5. How to Adjust Recall vs. Precision?**\n",
    "- **Increase Recall:** Lower the classification threshold (predict \"positive\" more liberally).  \n",
    "- **Increase Precision:** Raise the threshold (only predict \"positive\" with high confidence).  \n",
    "\n",
    "**Example in Spam Detection:**  \n",
    "- **For security (high recall):** Classify even slightly suspicious emails as spam.  \n",
    "- **For business (high precision):** Only block emails that are almost certainly spam.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**\n",
    "| **Priority** | **Optimize For** | **Cost of Error**               | **Example**                |\n",
    "|--------------|------------------|---------------------------------|----------------------------|\n",
    "| **Recall**   | Minimize FN      | Missing a positive is costly    | Cancer tests, fraud detection |\n",
    "| **Precision**| Minimize FP      | False alarms are costly         | Email spam (business), legal checks |\n",
    "| **F1-Score** | Balance both     | Both errors matter moderately   | Customer churn, defect detection |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35011f",
   "metadata": {},
   "source": [
    "### **How to Use Precision, Recall, and F1-Score to Evaluate Model Performance**  \n",
    "\n",
    "When evaluating a classification model (e.g., spam detection, cancer diagnosis), **precision, recall, and F1-score** help assess performance beyond just accuracy. But **whether \"larger is better\" depends on your goal**. Here’s how to interpret and use them:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Key Definitions & Formulas**\n",
    "| Metric      | Formula                          | Focus                     |\n",
    "|-------------|----------------------------------|---------------------------|\n",
    "| **Precision** | $$\\frac{TP}{TP + FP}$$        | **How many predicted positives are correct?** (Avoid FP) |\n",
    "| **Recall**    | $$\\frac{TP}{TP + FN}$$        | **How many actual positives were caught?** (Avoid FN) |\n",
    "| **F1-Score**  | $$2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$ | **Balances precision and recall** |\n",
    "\n",
    "---\n",
    "\n",
    "## **2. When to Prefer Higher Values?**\n",
    "### **(A) \"Larger is Better\" Depends on the Problem**\n",
    "| **Goal**                | **Optimize For** | **When to Use** |\n",
    "|-------------------------|------------------|----------------|\n",
    "| **Avoid False Alarms (FP)** | **High Precision** | Spam filtering (business), legal docs |\n",
    "| **Avoid Missed Cases (FN)**  | **High Recall**    | Cancer detection, fraud prevention |\n",
    "| **Balance FP & FN**         | **High F1-Score** | Moderate-risk scenarios (e.g., customer churn) |\n",
    "\n",
    "### **(B) Trade-offs**\n",
    "- **↑ Precision → ↓ Recall** (Fewer FP but more FN)  \n",
    "- **↑ Recall → ↓ Precision** (Fewer FN but more FP)  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. How to Evaluate Model Performance?**\n",
    "### **Step 1: Define Business Impact**\n",
    "- **Is a False Positive (FP) worse?** → Optimize **Precision**.  \n",
    "  - *Example:* Blocking a legitimate email (FP) is bad.  \n",
    "- **Is a False Negative (FN) worse?** → Optimize **Recall**.  \n",
    "  - *Example:* Missing cancer (FN) is deadly.  \n",
    "- **Need a balance?** → Use **F1-Score**.  \n",
    "\n",
    "### **Step 2: Check the Confusion Matrix**\n",
    "|                     | **Actual Positive** | **Actual Negative** |\n",
    "|---------------------|---------------------|---------------------|\n",
    "| **Predicted Positive** | True Positive (TP)  | False Positive (FP) |\n",
    "| **Predicted Negative** | False Negative (FN) | True Negative (TN) |\n",
    "\n",
    "- **High Recall?** → FN should be **low**.  \n",
    "- **High Precision?** → FP should be **low**.  \n",
    "\n",
    "### **Step 3: Adjust the Classification Threshold**\n",
    "- **To increase Recall:** Lower the threshold (predict more positives).  \n",
    "- **To increase Precision:** Raise the threshold (only predict positives with high confidence).  \n",
    "\n",
    "### **Step 4: Compare Models**\n",
    "- If **FN is costly**, pick the model with **highest recall**.  \n",
    "- If **FP is costly**, pick the model with **highest precision**.  \n",
    "- If both matter, pick the model with **best F1-score**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Real-World Examples**\n",
    "### **Example 1: Spam Detection**\n",
    "- **Goal:** Avoid blocking important emails (FP).  \n",
    "- **Optimize:** **Precision** (even if some spam slips through).  \n",
    "- **Evaluation:**  \n",
    "  - Precision = 95% → Only 5% of flagged emails are mistakes.  \n",
    "  - Recall = 70% → 30% of spam reaches the inbox (acceptable).  \n",
    "\n",
    "### **Example 2: Cancer Screening**\n",
    "- **Goal:** Catch all cancers (FN).  \n",
    "- **Optimize:** **Recall** (even if healthy patients get extra tests).  \n",
    "- **Evaluation:**  \n",
    "  - Recall = 98% → Only 2% of cancers missed.  \n",
    "  - Precision = 60% → 40% of \"positive\" tests are false alarms (acceptable).  \n",
    "\n",
    "### **Example 3: Fraud Detection**\n",
    "- **Goal:** Balance fraud catches (FN) and false accusations (FP).  \n",
    "- **Optimize:** **F1-Score**.  \n",
    "- **Evaluation:**  \n",
    "  - F1 = 85% → Good balance between catching fraud and minimizing false alerts.  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Summary: How to Choose?**\n",
    "| **Scenario**               | **Priority Metric** | **Why?** |\n",
    "|----------------------------|---------------------|----------|\n",
    "| **Medical diagnosis**      | **Recall**          | Missing a disease (FN) is worse than false alarms (FP). |\n",
    "| **Spam filtering (business)** | **Precision**      | Blocking legitimate emails (FP) is worse than missing spam (FN). |\n",
    "| **Fraud detection**        | **F1-Score**        | Need a balance—too many FP annoys users, too many FN loses money. |\n",
    "\n",
    "### **Final Rule of Thumb**\n",
    "- **\"Larger is better\" for the metric that aligns with your goal.**  \n",
    "- **Trade-offs exist:** You can’t maximize both precision and recall at the same time.  \n",
    "- **Use F1 when both FP and FN matter.**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9e5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
